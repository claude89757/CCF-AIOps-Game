#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
@author: claude89757
@date: 2025-06-29
@description: æ–‡ä»¶å‘çŽ°å™¨ - é€‚é…é¢„å¤„ç†åŽçš„æ•°æ®ç»“æž„
"""

import os
import re
import glob
import logging
from datetime import datetime
from typing import List, Dict

from ..config import AgentConfig


class FileDiscovery:
    """æ–‡ä»¶å‘çŽ°å™¨ - è´Ÿè´£ä»Žæ•…éšœæè¿°ä¸­å‘çŽ°ç›¸å…³æ–‡ä»¶"""
    
    def __init__(self, config: AgentConfig, loggers: Dict[str, logging.Logger]):
        self.config = config
        self.loggers = loggers
    
    def discover_relevant_files(self, description: str, debug: bool = False) -> str:
        """
        ä»Žæ•…éšœæè¿°ä¸­æå–æ—¶é—´çª—å£å¹¶å‘çŽ°ç›¸å…³æ–‡ä»¶
        çŽ°åœ¨æ•°æ®å·²é¢„å¤„ç†ï¼Œæ—¶é—´æˆ³å’Œæ–‡ä»¶å¤¹æ—¥æœŸä¸€è‡´ï¼ˆUTCæ—¶åŒºï¼‰
        
        Args:
            description: æ•…éšœæè¿°
            debug: æ˜¯å¦æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
            
        Returns:
            åŒ…å«æ–‡ä»¶ä¿¡æ¯çš„å­—ç¬¦ä¸²
        """
        try:
            self.loggers['diagnosis'].info("Start discovering relevant files...")
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ—¶é—´ä¿¡æ¯
            time_pattern = r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}Z)'
            times = re.findall(time_pattern, description)
            
            if len(times) >= 2:
                start_time = times[0]
                end_time = times[1]
                
                self.loggers['diagnosis'].info(f"Extracted time window: {start_time} to {end_time}")
                
                # è§£æžæ—¶é—´å¹¶æå–æ—¥æœŸ
                start_dt = datetime.fromisoformat(start_time.replace('Z', '+00:00'))
                end_dt = datetime.fromisoformat(end_time.replace('Z', '+00:00'))
                start_date = start_dt.strftime('%Y-%m-%d')
                end_date = end_dt.strftime('%Y-%m-%d')
                
                return self._discover_files_for_date_range(start_date, end_date, start_time, end_time)
                
            else:
                # å¦‚æžœæ— æ³•æå–æ—¶é—´ï¼Œå°è¯•æå–æ—¥æœŸ
                date_pattern = r'(\d{4}-\d{2}-\d{2})'
                dates = re.findall(date_pattern, description)
                
                if dates:
                    target_date = dates[0]
                    return self._discover_files_for_single_date(target_date)
        
        except Exception as e:
            self.loggers['diagnosis'].error(f"File discovery failed: {e}")
            return f"âš ï¸ Unable to automatically discover files: {str(e)}. Please manually use preview_parquet_in_pd tool to explore data structure."
        
        return "âš ï¸ Unable to extract time information from description. Please manually use preview_parquet_in_pd tool to explore data structure."
    
    def _discover_files_for_date_range(self, start_date: str, end_date: str, start_time: str, end_time: str) -> str:
        """å‘çŽ°æ—¥æœŸèŒƒå›´å†…çš„ç›¸å…³æ–‡ä»¶"""
        # å‘çŽ°ç›¸å…³æ–‡ä»¶
        log_files = []
        metric_files = []
        trace_files = []
        
        # èŽ·å–æ‰€æœ‰å¯ç”¨æ—¥æœŸ
        available_dates = self._get_available_dates()
        
        # æ£€æŸ¥ç›®æ ‡æ—¥æœŸæ•°æ®æ˜¯å¦å­˜åœ¨
        processed_data_dir = f"{self.config.data_base_path}/processed_data/{start_date}"
        if not os.path.exists(processed_data_dir):
            # æ™ºèƒ½é€‰æ‹©æœ€æŽ¥è¿‘çš„æ—¥æœŸ
            if available_dates:
                best_match_date = self._find_best_matching_date(start_date, available_dates)
                self.loggers['diagnosis'].warning(f"Target date {start_date} has no data, using closest available date: {best_match_date}")
                
                # è°ƒæ•´æ—¶é—´çª—å£åˆ°åŒ¹é…æ—¥æœŸ
                adjusted_start = start_time.replace(start_date, best_match_date)
                adjusted_end = end_time.replace(start_date, best_match_date)
                
                return f"""âš ï¸ **Time data matching problem automatically corrected**
Target date: {start_date} (data does not exist)
Adjusted to: {best_match_date} (closest available date)
Adjusted time window: {adjusted_start} to {adjusted_end}

ðŸ’¡ **Analysis suggestions**:
1. Use the adjusted time window for analysis
2. Data is now UTC-aligned, so timestamps match folder dates
3. Focus on fault patterns rather than specific time points

Available dates: {', '.join(available_dates)}
Recommended data directory: {self.config.data_base_path}/processed_data/{best_match_date}/"""
            else:
                return "âš ï¸ No monitoring data found."
        
        # å‘çŽ°å…·ä½“æ–‡ä»¶
        log_files, metric_files, trace_files = self._scan_files_in_directory(processed_data_dir)
        
        self.loggers['diagnosis'].info(f"Found {len(log_files)} logs, {len(metric_files)} metrics, {len(trace_files)} traces")
        
        # æ ¼å¼åŒ–æ–‡ä»¶ä¿¡æ¯
        return self._format_file_info(start_time, end_time, start_date, log_files, metric_files, trace_files)
    
    def _discover_files_for_single_date(self, target_date: str) -> str:
        """å‘çŽ°å•ä¸ªæ—¥æœŸçš„ç›¸å…³æ–‡ä»¶"""
        self.loggers['diagnosis'].info(f"æå–åˆ°æ—¥æœŸ: {target_date}")
        
        # æ£€æŸ¥è¯¥æ—¥æœŸçš„æ•°æ®æ˜¯å¦å­˜åœ¨
        processed_data_dir = f"{self.config.data_base_path}/processed_data/{target_date}"
        if not os.path.exists(processed_data_dir):
            # æŸ¥æ‰¾å¯ç”¨çš„æ—¥æœŸ
            available_dates = self._get_available_dates()
            
            if available_dates:
                available_dates = sorted(available_dates)
                return f"âš ï¸ Date {target_date} has no data. Available dates: {', '.join(available_dates)}"
            else:
                return "âš ï¸ No monitoring data found."
        
        return f"## Available monitoring data files\nTarget date: {target_date}\nðŸ’¡ Tip: Use preview_parquet_in_pd tool to explore specific files."
    
    def _get_available_dates(self) -> List[str]:
        """èŽ·å–æ‰€æœ‰å¯ç”¨çš„æ•°æ®æ—¥æœŸ"""
        available_dates = []
        pattern = f"{self.config.data_base_path}/processed_data/2025-*"
        for date_dir in glob.glob(pattern):
            if os.path.isdir(date_dir):
                date_name = os.path.basename(date_dir)
                available_dates.append(date_name)
        return sorted(available_dates)
    
    def _find_best_matching_date(self, target_date: str, available_dates: List[str]) -> str:
        """æ™ºèƒ½å¯»æ‰¾æœ€ä½³åŒ¹é…çš„æ•°æ®æ—¥æœŸ"""
        try:
            target_dt = datetime.strptime(target_date, '%Y-%m-%d')
            available_dts = []
            
            for date_str in available_dates:
                try:
                    dt = datetime.strptime(date_str, '%Y-%m-%d')
                    available_dts.append((dt, date_str))
                except:
                    continue
            
            if not available_dts:
                return available_dates[0] if available_dates else target_date
            
            # æ‰¾åˆ°æ—¶é—´å·®æœ€å°çš„æ—¥æœŸ
            best_match = min(available_dts, key=lambda x: abs((x[0] - target_dt).days))
            return best_match[1]
            
        except:
            return available_dates[0] if available_dates else target_date
    
    def _scan_files_in_directory(self, data_dir: str) -> tuple:
        """æ‰«æç›®å½•ä¸­çš„æ–‡ä»¶ - é€‚é…æ–°çš„æ•°æ®ç»“æž„"""
        log_files = []
        metric_files = []
        trace_files = []
        
        # å‘çŽ°æ—¥å¿—æ–‡ä»¶
        log_pattern = f"{data_dir}/log-parquet/*.parquet"
        log_files = sorted(glob.glob(log_pattern))
        
        # å‘çŽ°è°ƒç”¨é“¾æ–‡ä»¶
        trace_pattern = f"{data_dir}/trace-parquet/*.parquet"
        trace_files = sorted(glob.glob(trace_pattern))
        
        # å‘çŽ°æŒ‡æ ‡æ–‡ä»¶ - æ–°çš„æ‰å¹³åŒ–ç»“æž„
        # APMæŒ‡æ ‡
        apm_pattern = f"{data_dir}/apm/*.parquet"
        metric_files.extend(glob.glob(apm_pattern))
        
        # PodæŒ‡æ ‡
        pod_pattern = f"{data_dir}/pod/*.parquet"
        metric_files.extend(glob.glob(pod_pattern))
        
        # ServiceæŒ‡æ ‡
        service_pattern = f"{data_dir}/service/*.parquet"
        metric_files.extend(glob.glob(service_pattern))
        
        # åŸºç¡€è®¾æ–½æŒ‡æ ‡
        infra_patterns = [
            f"{data_dir}/infra_node/*.parquet",
            f"{data_dir}/infra_pod/*.parquet",
            f"{data_dir}/infra_tidb/*.parquet"
        ]
        for pattern in infra_patterns:
            metric_files.extend(glob.glob(pattern))
        
        # å…¶ä»–æŒ‡æ ‡
        other_pattern = f"{data_dir}/other/*.parquet"
        metric_files.extend(glob.glob(other_pattern))
        
        metric_files = sorted(metric_files)
        
        return log_files, metric_files, trace_files
    
    def _format_file_info(self, start_time: str, end_time: str, start_date: str, 
                         log_files: List[str], metric_files: List[str], trace_files: List[str]) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶ä¿¡æ¯"""
        file_info_parts = [
            "## Available monitoring data files (UTC-aligned)",
            f"Time window: {start_time} to {end_time}",
            f"Related date: {start_date}",
            f"File statistics: {len(log_files)} logs, {len(metric_files)} metrics, {len(trace_files)} traces"
        ]
        
        if log_files:
            file_info_parts.append("\n### Log files:")
            for log_file in log_files[:self.config.preview_rows]:
                file_info_parts.append(f"- {log_file}")
            if len(log_files) > self.config.preview_rows:
                file_info_parts.append(f"- ... and {len(log_files) - self.config.preview_rows} more logs")
        
        if trace_files:
            file_info_parts.append("\n### Trace files:")
            for trace_file in trace_files[:self.config.preview_rows]:
                file_info_parts.append(f"- {trace_file}")
            if len(trace_files) > self.config.preview_rows:
                file_info_parts.append(f"- ... and {len(trace_files) - self.config.preview_rows} more traces")
        
        if metric_files:
            file_info_parts.append("\n### Metric files:")
            # æŒ‰ç±»åž‹åˆ†ç»„æ˜¾ç¤ºæŒ‡æ ‡æ–‡ä»¶
            metric_groups = self._group_metric_files(metric_files)
            for group_name, files in metric_groups.items():
                file_info_parts.append(f"\n#### {group_name} ({len(files)} files):")
                for file in files[:3]:  # æ¯ç»„æ˜¾ç¤ºå‰3ä¸ªæ–‡ä»¶
                    file_info_parts.append(f"- {file}")
                if len(files) > 3:
                    file_info_parts.append(f"- ... and {len(files) - 3} more")
        
        file_info_parts.append("\nðŸ’¡ **Data is now UTC-aligned**: Timestamps in files match folder dates")
        file_info_parts.append("ðŸ’¡ **Next steps**: Use preview_parquet_in_pd tool to preview file structure, then use get_data_from_parquet to get specific data.")
        
        return "\n".join(file_info_parts)
    
    def _group_metric_files(self, metric_files: List[str]) -> Dict[str, List[str]]:
        """å°†æŒ‡æ ‡æ–‡ä»¶æŒ‰ç±»åž‹åˆ†ç»„"""
        groups = {
            "APM Metrics": [],
            "Pod Metrics": [],
            "Service Metrics": [],
            "Infrastructure Node": [],
            "Infrastructure Pod": [],
            "Infrastructure TiDB": [],
            "Other Metrics": []
        }
        
        for file in metric_files:
            if "/apm/" in file:
                groups["APM Metrics"].append(file)
            elif "/pod/" in file:
                groups["Pod Metrics"].append(file)
            elif "/service/" in file:
                groups["Service Metrics"].append(file)
            elif "/infra_node/" in file:
                groups["Infrastructure Node"].append(file)
            elif "/infra_pod/" in file:
                groups["Infrastructure Pod"].append(file)
            elif "/infra_tidb/" in file:
                groups["Infrastructure TiDB"].append(file)
            elif "/other/" in file:
                groups["Other Metrics"].append(file)
        
        # ç§»é™¤ç©ºç»„
        return {k: v for k, v in groups.items() if v} 